{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4CMx98e0CaCXcLolmUTy1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kimuksung/bigdata/blob/master/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7ba5Y7GxeVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "MNIST + CNN basic\n",
        "    - Convolution layer : 특징맵(feature mat)\n",
        "    - Pooling layer : 픽셀 축소(down sampling), 특징 강조  \n",
        "\n",
        "\"\"\"\n",
        "import tensorflow.compat.v1 as tf # ver1.x\n",
        "tf.disable_v2_behavior() # ver2.0 사용안함\n",
        "\n",
        "from tensorflow.keras.datasets.mnist import load_data # dataset load\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. image read \n",
        "(x_train, y_train), (x_test, y_test) = load_data()\n",
        "print(x_train.shape) # (60000, 28, 28)\n",
        "print(y_train.shape) # (60000,) : 10진수 \n",
        "print(x_test.shape) # (10000, 28, 28)\n",
        "print(y_test.shape) # (10000,) : 10진수 \n",
        "print(x_train[0]) # 0 ~ 255\n",
        "\n",
        "# 2. 실수형 변환 : int -> float32\n",
        "x_train = x_train.astype('float32') \n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# 3. 정규화 \n",
        "x_train = x_train / 255 # x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "# first image \n",
        "img = x_train[0]\n",
        "plt.imshow(img, cmap='gray') # 숫자 5  \n",
        "\n",
        "# input image reshape  \n",
        "firstImg = img.reshape(1,28,28,1) #(size , h , w, color)\n",
        "\n",
        "# Filter 변수 정의 \n",
        "Filter = tf.Variable(tf.random_normal([3,3,1,5])) # 난수 (row , column , color ,fmap) \n",
        " \n",
        "# 1. Convolution layer : 특징 추출\n",
        "conv2d = tf.nn.conv2d(firstImg, Filter, strides=[1,1,1,1], padding='SAME')\n",
        "print(conv2d) \n",
        "\n",
        "# 2. Pool layer : down sampling\n",
        "pool = tf.nn.max_pool(conv2d, ksize=[1,2,2,1],strides=[1,2,2,1],\n",
        "            padding = 'SAME')\n",
        "print(pool) \n",
        "\n",
        "\n",
        "with tf.Session() as sess :\n",
        "    sess.run(tf.global_variables_initializer()) # filter 초기화 \n",
        "    \n",
        "    # 합성곱 연산 \n",
        "    conv2d_img = sess.run(conv2d)    \n",
        "    conv2d_img = np.swapaxes(conv2d_img, 0, 3) # 축 교환 \n",
        "    print(\"this:\" , conv2d_img.shape) # (5, 28, 28, 1)\n",
        "    \n",
        "    for i, img in enumerate(conv2d_img) :\n",
        "        plt.subplot(1, 5, i+1) # 1행5열,1~5 \n",
        "        plt.imshow(img.reshape(28,28), cmap='gray') # \n",
        "    plt.show()\n",
        "    \n",
        "    # 폴링 연산 \n",
        "    pool_img = sess.run(pool)\n",
        "    pool_img = np.swapaxes(pool_img, 0, 3)\n",
        "    \n",
        "    for i, img in enumerate(pool_img) :\n",
        "        plt.subplot(1,5, i+1) # 1행5열,1~5 \n",
        "        plt.imshow(img.reshape(14,14), cmap='gray') \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6KIQ0RgxrBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "\n",
        "- Keras CNN model + cifar10 \n",
        "\n",
        "1. image dataset load\n",
        "2. image preprocessing : 실수형 , 정규화 , one-hot-encoding\n",
        "3. Keras Model\n",
        "4. Model evaluate\n",
        "5. Model history\n",
        "\n",
        "'''\n",
        "\n",
        "# keras dataset 적용\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets.cifar10 import load_data\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Conv2D , MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense , Flatten , Dropout\n",
        "\n",
        "\n",
        "# 1. dataset load\n",
        "(x_train , y_train ) , (x_val , y_val) = load_data()\n",
        "x_train.shape # (50000, 32, 32, 3)\n",
        "y_train.shape # (50000, 1)\n",
        "\n",
        "# image 전처리 : 실수형 -> 정규화\n",
        "x_train[0] # 0 ~ 255 : 정수형\n",
        "x_train = x_train.astype(\"float32\")\n",
        "x_val = x_val.astype(\"float32\")\n",
        "\n",
        "# 정규화\n",
        "x_train = x_train / 255\n",
        "x_val = x_val / 255\n",
        "x_train[0]\n",
        "\n",
        "# label 전처리 : one-hot\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)\n",
        "\n",
        "# 2. Model\n",
        "# [5,5,3,32] : kernel_size -> Filter\n",
        "input_shape = (x_train.shape[1] , x_train.shape[2], x_train.shape[3])\n",
        "\n",
        "# conv layer1\n",
        "model = Sequential()\n",
        "model.add(Conv2D( 32 , kernel_size = (5,5) , input_shape = input_shape , activation = \"relu\" ))\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides = (2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# conv layer2 : [5,5,32,64]\n",
        "model.add(Conv2D( 64 , kernel_size = (5,5) , activation = \"relu\" ))\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides = (2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Flatten : 3d -> 1d\n",
        "model.add(Flatten())\n",
        "\n",
        "# DNN layer\n",
        "model.add(Dense(64, activation = \"relu\" ))\n",
        "\n",
        "# DNN output layer\n",
        "model.add(Dense(10, activation = \"softmax\" ))\n",
        "\n",
        "# 3. Model evnironment setting\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# 4. model training\n",
        "model_fit = model.fit( x= x_train, y=y_train , batch_size = 100 ,epochs=10 , verbose=1 , validation_data = (x_val , y_val))\n",
        "\n",
        "# 5. model evaluation\n",
        "model.evaluate( x = x_val , y=y_val)\n",
        "\n",
        "\n",
        "\n",
        "labels = [ \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
        "\n",
        "# 6. model history\n",
        "model_fit.history.keys()\n",
        "train_loss = model_fit.history['loss']\n",
        "train_acc = model_fit.history['accuracy']\n",
        "val_loss = model_fit.history['val_loss']\n",
        "val_acc = model_fit.history['val_accuracy']\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_loss , label = 'train loss',color = 'y' )\n",
        "plt.plot(val_loss , label = 'val loss' , color='r')\n",
        "plt.legend(loc='best')\n",
        "plt.show\n",
        "\n",
        "\n",
        "plt.plot(train_acc , label = 'train_acc loss',color = 'y' )\n",
        "plt.plot(val_acc , label = 'val val_acc' , color='r')\n",
        "plt.legend(loc='best')\n",
        "plt.show\n",
        "\n",
        "# 7. model test ( new data set )\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import numpy as np\n",
        "idx = np.random.choice(a = x_val.shape[0] ,size = 100 , replace = False)\n",
        "x_test = x_val[idx] # new dataset images\n",
        "y_test = y_val[idx] # new dataset labels\n",
        "\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred = np.argmax(y_pred , 1)\n",
        "y_true = np.argmax(y_test , 1)\n",
        "\n",
        "report = classification_report(y_true, y_pred  )\n",
        "print(report)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNqxta-dxgcm",
        "colab_type": "text"
      },
      "source": [
        "# CNN\n",
        "\n",
        "- convolution newlearn network\n",
        "- 이미지와 같은 큰 data를 fully connected layer 방식으로 하면 엄청난 연산이 필요하며 모든 feature를 나타낸다.\n",
        "- locally connected 개념을 이용하여 filter \n",
        "- filterling을 통해 전체를 보지 않고 특정 영역만 본다.\n",
        "-이미지의 특성상 같은 object라도 회전하거나 뒤집으면 DNN(1차원)은 다르게 보지만 CNN은 filter의 feature를 보기 때문에 이를 동일하게 본다.\n",
        "- max pooling은 locally connected된 feature 중에서 가장 영향이 큰 feature를 선택한 것이다. 최근에는 이 때문에 pooling을 하지 않는다.\n",
        "\n",
        "\n",
        "## 1. convolution : input x filter -> feature\n",
        "\n",
        "<img src=\"http://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif\" alt=\"합성곱 처리 절치, 출처: http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution\" style=\"border:2px solid black\">\n",
        "\n",
        "## 2. filter : window size 개념으로 특정 영역을 얼만큼 보겠다.\n",
        "\n",
        "## 3. stride : filter를 얼만큼씩 움직이겠다.\n",
        "\n",
        "## 4. padding : 데이터가 줄어드는 것을 방지하는 방법이 패딩\n",
        "<img src=\"https://taewanmerepo.github.io/2018/01/cnn/conv2.jpg\" alt=\"멀티 채널 입력 데이터에 필터를 적용한 합성곱 계산 절차\" style=\"border:2px solid black\">\n",
        "\n",
        "## 5. pooling :  특정 데이터를 강조하는 용도로 사용되어 특정 feature를 나타낸다."
      ]
    }
  ]
}