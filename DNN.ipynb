{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4PfTOskc0gcACS8Ethz+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kimuksung/bigdata/blob/master/DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqHTQb2xSy-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "45787581-6ac7-430b-936f-3c2705c7aa85"
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "DNN model\n",
        "    - hidden layer : relu function\n",
        "    - output layer : Softmax activation function\n",
        "    - one hidden layer classifier\n",
        "    - node : 5\n",
        "    - data set : iris\n",
        "    \n",
        "\"\"\"\n",
        "\n",
        "import tensorflow.compat.v1 as tf # ver1.x\n",
        "tf.disable_v2_behavior() # ver2.0 사용안함\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# x, y data\n",
        "iris = load_iris() \n",
        "x_data = iris.data\n",
        "y_data = iris.target\n",
        "y_data = y_data.reshape(-1,1)\n",
        "\n",
        "obj = OneHotEncoder()\n",
        "# sparse -> numpy\n",
        "y_data = obj.fit_transform(y_data).toarray()\n",
        "y_data.shape\n",
        "\n",
        "# x, y 변수 정의\n",
        "X = tf.placeholder(dtype = tf.float32 , shape =[None , 4])\n",
        "Y = tf.placeholder(dtype = tf.float32 , shape =[None , 3])\n",
        "\n",
        "# =============================================================================\n",
        "# Dnn network\n",
        "# =============================================================================\n",
        "\n",
        "hidden_node = 5\n",
        "\n",
        "# hidden layer\n",
        "w1 = tf.Variable(tf.random_normal([ 4 , hidden_node]))\n",
        "b1 = tf.Variable(tf.random_normal([ hidden_node ]))\n",
        "\n",
        "# output layer \n",
        "w2 = tf.Variable(tf.random_normal([ hidden_node , 3]))\n",
        "b2 = tf.Variable(tf.random_normal([ 3 ]))\n",
        "\n",
        "# softmax 분류기 \n",
        "hidden_output = tf.nn.relu(tf.matmul(X, w1) + b1) # hidden layer \n",
        "\n",
        "model = tf.matmul(hidden_output , w2) + b2                           \n",
        "                           \n",
        "# softmax(예측치)\n",
        "softmax = tf.nn.softmax(model) \n",
        "\n",
        "# loss function : Entropy 이용 : -sum(Y * log(model)) \n",
        "# softmax  + crossentrophy\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "    labels = Y , logits = model\n",
        "    ))\n",
        "\n",
        "# optimizer : 오차 최소화(w, b update) \n",
        "train = tf.train.AdamOptimizer(0.1).minimize(loss) # 오차 최소화\n",
        "\n",
        "# argmax() : encoding -> decoding\n",
        "y_pred = tf.argmax(softmax , axis =1)\n",
        "y_true = tf.argmax(Y , axis =1)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    feed_data= { X : x_data , Y : y_data}\n",
        "    \n",
        "    #반복 학습\n",
        "    for step in range(500):\n",
        "        _, loss_val = sess.run([train , loss] , feed_dict = feed_data)\n",
        "        \n",
        "        if (step+1)% 50 == 0 :\n",
        "            print(\" step : {} , loss : {}\".format(step+1 , loss_val))\n",
        "\n",
        "    y_pred_re = sess.run( y_pred , feed_dict ={ X :x_data })\n",
        "    y_true_re = sess.run(y_true , feed_dict = { Y : y_data })\n",
        "    \n",
        "    print(\"y pred = \" , y_pred_re)\n",
        "    print(\"y true = \" , y_true_re)\n",
        "    acc = accuracy_score(y_true_re,y_pred_re)\n",
        "    print(\"acc = \" , acc)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            " step : 50 , loss : 0.4927338659763336\n",
            " step : 100 , loss : 0.16919703781604767\n",
            " step : 150 , loss : 0.09525299817323685\n",
            " step : 200 , loss : 0.07512073963880539\n",
            " step : 250 , loss : 0.06545151770114899\n",
            " step : 300 , loss : 0.05962446331977844\n",
            " step : 350 , loss : 0.05574227496981621\n",
            " step : 400 , loss : 0.05301323160529137\n",
            " step : 450 , loss : 0.051008980721235275\n",
            " step : 500 , loss : 0.049476031213998795\n",
            "y pred =  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "y true =  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "acc =  0.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQzBxgAREhxL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "f546f69d-ae7e-4ab8-9d05-844dadc93856"
      },
      "source": [
        "import tensorflow.compat.v1 as tf # ver1.x\n",
        "tf.disable_v2_behavior() # ver2.0 사용안함\n",
        "from sklearn.datasets import load_wine # data set\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "# 1. wine data load\n",
        "wine = load_wine()\n",
        "\n",
        "# 2. 변수 선택/전처리  \n",
        "x_data = wine.data # 178x13\n",
        "y_data = wine.target # 3개 domain\n",
        "print(y_data) # 0-2\n",
        "print(x_data.shape) # (178, 13)\n",
        "\n",
        "# x_data : 정규화 \n",
        "x_data = minmax_scale(x_data) # 0~1\n",
        "\n",
        "# y변수 one-hot-encoding : 0=[1,0,0] / 1=[0,1,0] / 2=[0,0,1]\n",
        "num_class = np.max(y_data)+1 # 2+1\n",
        "print(num_class) # 3\n",
        "\n",
        "y_data = np.eye(num_class)[y_data]\n",
        "print(y_data.shape) # (178, 3)\n",
        "\n",
        "# 4. train/test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x_data, y_data, test_size=0.2, random_state=123)\n",
        "\n",
        "# 5. X,Y 변수 정의  \n",
        "X = tf.placeholder(tf.float32, shape=[None, 13]) # [n, 13개 원소]\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 3]) # [n, 3개 원소]\n",
        "\n",
        "# 6. Hypter parameters\n",
        "learning_rate = 0.01\n",
        "iter_size = 1000\n",
        "    \n",
        "##############################\n",
        "### DNN network\n",
        "# =============================================================================\n",
        "#   <조건1>   \n",
        "#    - Hidden layer : relu()함수 이용  \n",
        "#    - Output layer : softmax()함수 이용 \n",
        "#    - 2개의 은닉층을 갖는 DNN 분류기\n",
        "#      hidden1 : nodes = 6\n",
        "#      hidden2 : nodes = 3  \n",
        "#   <조건2> hyper parameters\n",
        "#     learning_rate = 0.01\n",
        "#     iter_size = 1,000\n",
        "#   <조건3>  \n",
        "#     train/test(80:20)\n",
        "#     x_data : 정규화 \n",
        "#     y_data : one-hot encoding\n",
        "# =============================================================================\n",
        "##############################\n",
        "\n",
        "hidden_node1 = 6\n",
        "hidden_node2 = 3\n",
        "\n",
        "# hidden layer1\n",
        "w1 = tf.Variable(tf.random_normal([ 13 , hidden_node1]))\n",
        "b1 = tf.Variable(tf.random_normal([ hidden_node1 ]))\n",
        "\n",
        "# hidden layer2\n",
        "w2 = tf.Variable(tf.random_normal([ hidden_node1 , hidden_node2]))\n",
        "b2 = tf.Variable(tf.random_normal([ hidden_node2 ]))\n",
        "\n",
        "# output layer \n",
        "w3 = tf.Variable(tf.random_normal([ hidden_node2 , 3]))\n",
        "b3 = tf.Variable(tf.random_normal([ 3 ]))\n",
        "\n",
        "hidden_output1 = tf.nn.relu(tf.matmul(X, w1) + b1) # hidden layer \n",
        "hidden_output2 = tf.nn.relu(tf.matmul(hidden_output1, w2) + b2)\n",
        "model = tf.matmul(hidden_output2 , w3) + b3     \n",
        "softmax = tf.nn.softmax(model) \n",
        "\n",
        "\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "    labels = Y , logits = model\n",
        "    ))\n",
        "train = tf.train.AdamOptimizer(0.1).minimize(loss)\n",
        "\n",
        "y_pred = tf.argmax(softmax , axis =1)\n",
        "y_true = tf.argmax(Y , axis =1)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    feed_data= { X : x_train , Y : y_train}\n",
        "    \n",
        "    #반복 학습\n",
        "    for step in range(500):\n",
        "        _, loss_val = sess.run([train , loss] , feed_dict = feed_data)\n",
        "        \n",
        "        if (step+1)% 50 == 0 :\n",
        "            print(\" step : {} , loss : {}\".format(step+1 , loss_val))\n",
        "\n",
        "    # model result\n",
        "    y_pred_re = sess.run( y_pred , feed_dict ={ X :x_test })\n",
        "    y_true_re = sess.run(y_true , feed_dict = { Y : y_test })\n",
        "    \n",
        "    print(\"y pred = \" , y_pred_re)\n",
        "    print(\"y true = \" , y_true_re)\n",
        "    acc = accuracy_score(y_true_re,y_pred_re)\n",
        "    print(\"acc = \" , acc)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
            "(178, 13)\n",
            "3\n",
            "(178, 3)\n",
            " step : 50 , loss : 0.11532185971736908\n",
            " step : 100 , loss : 0.007790032308548689\n",
            " step : 150 , loss : 0.0038279995787888765\n",
            " step : 200 , loss : 0.002464162651449442\n",
            " step : 250 , loss : 0.0017523118294775486\n",
            " step : 300 , loss : 0.0013253113720566034\n",
            " step : 350 , loss : 0.0010461151832714677\n",
            " step : 400 , loss : 0.000850092910695821\n",
            " step : 450 , loss : 0.0007073901942931116\n",
            " step : 500 , loss : 0.0005998757551424205\n",
            "y pred =  [2 1 2 1 1 2 0 2 2 1 2 2 2 0 0 2 1 1 0 1 1 2 2 2 1 2 2 1 0 0 0 0 1 1 2 1]\n",
            "y true =  [2 1 2 1 1 2 0 2 2 1 2 2 2 0 0 2 1 1 0 1 2 2 2 2 1 2 2 1 0 0 0 0 2 1 2 1]\n",
            "acc =  0.9444444444444444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCqySL7gUvo_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "ba5d9cfd-4b91-414b-d950-642e3edbf1c6"
      },
      "source": [
        "import tensorflow.compat.v1 as tf # ver 1.x\n",
        "tf.disable_v2_behavior() # ver 2.x 사용안함\n",
        "from sklearn.preprocessing import OneHotEncoder # y data -> one hot\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "tf.set_random_seed(123) # w,b seed\n",
        "\n",
        "# 1. MNIST dataset load\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# train set shape 확인 \n",
        "x_train.shape # (60000, 28, 28) -> image(입력) : (size, h, w)\n",
        "y_train.shape # (60000,) -> label(정답)\n",
        "\n",
        "# test set shape\n",
        "x_test.shape # (10000, 28, 28) -> image(입력)\n",
        "y_test.shape # (10000,) -> label(정답)\n",
        "\n",
        "# 2. 전처리 : X변수 정규화, Y변수 one-hot encoding  \n",
        "x_train_nor, x_test_nor = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# one-hot encoding\n",
        "obj = OneHotEncoder()\n",
        "train_labels = obj.fit_transform(y_train.reshape([-1, 1])).toarray()\n",
        "train_labels.shape # (60000, 10)\n",
        "\n",
        "test_labels = obj.fit_transform(y_test.reshape([-1, 1])).toarray()\n",
        "test_labels.shape # (10000, 10)\n",
        "\n",
        "\n",
        "# 3. 공급 data : image reshape(3d -> 2d)\n",
        "train_images = x_train_nor.reshape(60000, 784)\n",
        "test_images = x_test_nor.reshape(10000, 784)\n",
        "train_images.shape # (60000, 784)\n",
        "test_images.shape # (10000, 784)\n",
        "\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, 784]) # [관측치, 입력수]\n",
        "Y = tf.placeholder(tf.float32, [None, 10]) # [관측치, 출력수]\n",
        "\n",
        "# hyper parameters\n",
        "lr = 0.01\n",
        "epochs = 15 # 전체 images(60,000) 20번 재사용 \n",
        "batch_size = 100 # 1회 data 공급 size\n",
        "iter_size = 600 # 반복횟수 \n",
        "\n",
        "##############################\n",
        "### MNIST DNN network\n",
        "##############################\n",
        "# =============================================================================\n",
        "#   조건1> hyper parameters\n",
        "#     learning_rate = 0.01\n",
        "#     training_epochs = 15\n",
        "#     batch_size = 100\n",
        "#     iter_size = 600\n",
        "#   조건2> DNN layer\n",
        "#     Layer1 =  784 x 512\n",
        "#     Layer2 =  512 x 256\n",
        "#     Layer3 =  256 x 10 \n",
        "# =============================================================================\n",
        "\n",
        "input_size = train_images.shape[1]\n",
        "hidden_node1 = 512\n",
        "hidden_node2 = 256\n",
        "output_size =10\n",
        "\n",
        "# hidden layer1\n",
        "w1 = tf.Variable(tf.random_normal([ input_size , hidden_node1]))\n",
        "b1 = tf.Variable(tf.random_normal([ hidden_node1 ]))\n",
        "\n",
        "# hidden layer2\n",
        "w2 = tf.Variable(tf.random_normal([ hidden_node1 , hidden_node2]))\n",
        "b2 = tf.Variable(tf.random_normal([ hidden_node2 ]))\n",
        "\n",
        "# hidden layer3\n",
        "w3 = tf.Variable(tf.random_normal([ hidden_node2 , hidden_node2]))\n",
        "b3 = tf.Variable(tf.random_normal([ hidden_node2 ]))\n",
        "\n",
        "# output layer \n",
        "w4 = tf.Variable(tf.random_normal([ hidden_node2 , output_size]))\n",
        "b4 = tf.Variable(tf.random_normal([ output_size ]))\n",
        "\n",
        "hidden_output1 = tf.nn.relu(tf.matmul(X, w1) + b1) # hidden layer \n",
        "hidden_output2 = tf.nn.relu(tf.matmul(hidden_output1, w2) + b2)\n",
        "hidden_output3 = tf.nn.relu(tf.matmul(hidden_output2, w3) + b3)\n",
        "model = tf.matmul(hidden_output2 , w4) + b4                           \n",
        "softmax = tf.nn.softmax(model) \n",
        "\n",
        "# (3) loss function : Softmaxt + Cross Entorpy\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "        labels = Y, logits = model))\n",
        "\n",
        "# (4) optimizer \n",
        "train = tf.train.AdamOptimizer(lr).minimize(loss) \n",
        "\n",
        "# (5) encoding -> decoding \n",
        "y_pred = tf.argmax(softmax, axis = 1)\n",
        "y_true = tf.argmax(Y, axis = 1)\n",
        "\n",
        "# 6. model training\n",
        "with tf.Session() as sess :\n",
        "    sess.run(tf.global_variables_initializer()) # w, b 초기화 \n",
        "    for epoch in range(epochs) : \n",
        "        tot_loss = 0\n",
        "        for step in range(iter_size):\n",
        "            idx = np.random.choice( a = train_labels.shape[0] ,size = batch_size , replace =False)\n",
        "            feed_data = { X : train_images[idx], Y : train_labels[idx]}\n",
        "            _, loss_val = sess.run([train , loss] , feed_dict = feed_data)\n",
        "            \n",
        "            tot_loss += loss_val\n",
        "        #1epoch 종료\n",
        "        avg_loss = tot_loss / iter_size\n",
        "        print(\"epoch = {} , loss = {}\".format(epoch, avg_loss))\n",
        "        \n",
        "    # model test \n",
        "    feed_data2 = {X : test_images, Y : test_labels}\n",
        "    y_pred_re = sess.run(y_pred, feed_dict = feed_data2)\n",
        "    y_true_re = sess.run(y_true, feed_dict = feed_data2)\n",
        "    acc = accuracy_score(y_true_re, y_pred_re)\n",
        "    print(\"accuracy =\", acc)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "epoch = 0 , loss = 53.74276607250174\n",
            "epoch = 1 , loss = 9.815601193227712\n",
            "epoch = 2 , loss = 5.550342130366633\n",
            "epoch = 3 , loss = 3.6091982772822604\n",
            "epoch = 4 , loss = 2.929191029036694\n",
            "epoch = 5 , loss = 2.157738314670073\n",
            "epoch = 6 , loss = 2.1126319621956986\n",
            "epoch = 7 , loss = 1.7146935196972064\n",
            "epoch = 8 , loss = 1.3126343060453844\n",
            "epoch = 9 , loss = 1.5064779489444613\n",
            "epoch = 10 , loss = 1.1906491389160943\n",
            "epoch = 11 , loss = 0.9195669878533951\n",
            "epoch = 12 , loss = 0.8787473325132692\n",
            "epoch = 13 , loss = 0.68781101511618\n",
            "epoch = 14 , loss = 0.5746037287521939\n",
            "accuracy = 0.9603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zry-tAQnXwRL",
        "colab_type": "text"
      },
      "source": [
        "# DNN - tensorflow 1.x\n",
        "\n",
        "\n",
        "*   입력층(input layer)과 출력층(output layer) 사이에 여러 개의 은닉층(hidden layer) \n",
        "*   중간층의 다층화로 뉴런 처리와 전달, 산출되는 특징 값이 늘어남 → 정확도 향상\n",
        "*   파라미터 수가 너무 많아짐 → 연산 많아짐, 과적합\n",
        "*   완전연결 계층(Fully Connected NN)\n",
        "*  layer node 수 : 입력층과 가까울 수록 많게, 출력층과 가까울 수록 적게 지정\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# activation  : 활성화 함수\n",
        "\n",
        "\n",
        "1.   Sigmoid function\n",
        "\n",
        "*   0~1 까지 연속적으로 변화하는 출력값을 갖기 때문에 가중치나 바이어스(bias) 변화 시 출력에 변화를 준다\n",
        "\n",
        "*    이진 분류 문제에서 출력층에 주로 쓰입니다\n",
        "\n",
        "\n",
        "\n",
        "2.   ReLU function\n",
        "*    0보다 작을 때는 0을 사용, 0보다 큰 값에 대해서는 해당 값을 그대로 사용하는 방법\n",
        "\n",
        "*    은익층에 주로 쓰입니다\n",
        "\n",
        "3.   linear\n",
        "*    입력뉴런과 가중치로 계산된 결과값이 그대로 출력\n",
        "\n",
        "4.   softmax\n",
        "*    다중 클래스 분류 문제에서 출력층에 주로 쓰입니다.\n",
        "---\n",
        "# Loss\n",
        "* 현재 가중치 세트를 평가하는 데 사용한 손실 함수\n",
        "---\n",
        "# optimizer \n",
        "* 최적의 가중치를 검색하는 데 사용되는 최적화 알고리즘\n",
        "* ex) adam / SGD / RMSprop ..\n",
        "* loss를 이용해서 backpropagation을 해주어 가중치 조절\n",
        "\n",
        "<a href=\"https://ibb.co/xgFyJwS\"><img src=\"https://i.ibb.co/qgJzkhM/DNN.png\" alt=\"DNN\" border=\"0\"></a>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "참고 : https://tykimos.github.io/2017/01/27/MLP_Layer_Talk/"
      ]
    }
  ]
}