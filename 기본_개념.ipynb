{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "기본 개념.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2PhU1-XvvM2Q"
      ],
      "authorship_tag": "ABX9TyNKtSP4BYGoTVOjVgoLOlgf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kimuksung/bigdata/blob/master/%EA%B8%B0%EB%B3%B8_%EA%B0%9C%EB%85%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXg2thOFpuwF",
        "colab_type": "text"
      },
      "source": [
        "# 인공 신경망 Newlearn network 기본 개념\n",
        "\n",
        "- Newlearn network : 방향이 있는 그래프로써 information propagation 이 한 방향으로 진행된다. 양방향으로 진행된다면 information propagation이 recursive에 빠져 복잡해진다. 이를 RNN , LSTM 이라 하며, 최근 자연어 처리 및 시계열에 많이 사용되고 있다.\n",
        "\n",
        "- feed forward network : layer layer 사이에만 edge로 연결 / layer1 -> layer4 X\n",
        "Weight 와 Bias를 바탕으로 하나의 node가 activation에 맞추어 활성화 되면 해당 값을 전달하는 방식\n",
        "\n",
        "- 실제 뉴런에서는 Newlearn 각각의 Newlearn이 activate 되어 결과를 전달하여 최종 결정을 내리는 뉴런의 activate 값에 의해 행동된다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF2bVuMpstG3",
        "colab_type": "text"
      },
      "source": [
        "#Activation function\n",
        "\n",
        "Activation[ X1W1 + .. + XnWn + b]\n",
        "\n",
        "<img src=\"https://i.ibb.co/QXKpY9F/activation-function.png\" alt=\"activation-function\" border=\"0\">\n",
        "<br/>\n",
        "<img src=\"https://i.ibb.co/yBGpjxL/linear.png\" alt=\"linear\" border=\"0\">\n",
        "<img src=\"https://i.ibb.co/sKKPpW1/sigmoid.png\" alt=\"sigmoid\" border=\"0\">\n",
        "<img src=\"https://i.ibb.co/6yCm9tV/relu.png\" alt=\"relu\" border=\"0\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2ZvckdosxMU",
        "colab_type": "text"
      },
      "source": [
        "# Back propagation\n",
        "\n",
        "- activation function이 non-linear하며 layer끼리 연결되어 weight optimization은 non-convex하다. 따라서 일반적인 경우 최적의 optimize를 찾는 것이 매우 어렵기 때문에 최선의 방법이 gradient 기법 사용\n",
        "\n",
        "- 모든 optimization 문제는 target function이 정의되어야 풀 수 있다. 따라서 loss function을 이용하여 target output을 맞추는데 초점을 두어 minimize한다.\n",
        "\n",
        "- backpropagation algorithm은 chian rule을 이용하면 매우 간단히 만들 수 있다.\n",
        "\n",
        "- sigmoid function 에서 0과 1에 가까운 값에서는 backpropagation이 제대로 전달되지 않는 vanishing gradient 문제가 발생한다.\n",
        "\n",
        "<img src=\"https://i.ibb.co/YW2tGTq/backpropagation.png\" alt=\"backpropagation\" border=\"0\">\n",
        "\n",
        "# loss function\n",
        "\n",
        "<img src=\"https://i.ibb.co/0Dts6tV/loss-function.png\" alt=\"loss-function\" border=\"0\">\n",
        "\n",
        "# Optimizer\n",
        "- gd = full batch / 너무 오래 걸린다.\n",
        "\n",
        "- sgd = mini batch / gd를 보완(시간을 줄인다.)\n",
        "\n",
        "\n",
        "<img src=\"https://i.ibb.co/mFj4Ww0/optimizer.png\" alt=\"optimizer\" border=\"0\">\n",
        "\n",
        "# Algorithm\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://i.ibb.co/Pg7xD45/backpropagation-algo.png\" alt=\"backpropagation-algo\" border=\"0\">\n",
        "\n",
        "<img src=\"https://i.ibb.co/wYcrQLS/backpropagation-algo1.png\" alt=\"backpropagation-algo1\" border=\"0\">\n",
        "\n",
        "<img src=\"https://i.ibb.co/vYMzc9q/backpropagation-algo2.png\" alt=\"backpropagation-algo2\" border=\"0\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PhU1-XvvM2Q",
        "colab_type": "text"
      },
      "source": [
        "#overfitting\n",
        "  \n",
        "overfitting : 학습을 하다보면 train loss는 점차 줄어들기 마련인데, 학습을 과하게 하다보면 세세한 영역도 학습하기 시작하여 원래 목표와는 어긋나는 경우가 발생\n",
        "이를 방지하기 위해 train loss 와 val loss가 둘다 감소하면 underfitting / train loss가 감소 , val loss가 증가하면 overfitting이라 한다.\n",
        "\n",
        "- 원인\n",
        "\n",
        "1.   훈련데이터 적은 경우, 가중치가 큰 경우\n",
        "\n",
        "2.   Layer, Node 많은 모델(표현력이 높은 모델)\n",
        "\n",
        "- 해결 방안\n",
        "\n",
        "\n",
        "1.   가중치 감소(큰 가중치에 대한 패널티 부과 방법\n",
        " ) -> L2법칙(Ridge) : cost + 1/2λW2\n",
        "2.   드롭아웃 : 훈련 시 은닉층의 뉴런 무작위 삭제, 신호 전달 차단\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://i.ibb.co/6g2TTbM/overfitting1.png\" alt=\"overfitting1\" border=\"0\">\n",
        "<img src=\"https://i.ibb.co/ctBYBrN/overfitting2.png\" alt=\"overfitting2\" border=\"0\">\n",
        "<img src=\"https://i.ibb.co/vw5BVLC/overfitting3.png\" alt=\"overfitting3\" border=\"0\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa3C5RNbUTNX",
        "colab_type": "text"
      },
      "source": [
        "출처:\n",
        "http://sanghyukchun.github.io/74/\n",
        "https://seamless.tistory.com/38\n",
        "https://www.solver.com/convex-optimization"
      ]
    }
  ]
}